# SOME DESCRIPTIVE TITLE.
# Copyright (C) nndeploy
# This file is distributed under the same license as the nndeploy package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: nndeploy\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-05-10 17:38+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: en <LL@li.org>\n"
"Language: en\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"Generated-By: Babel 2.17.0\n"

#: ../../inference/README_INFERENCE.md:2 7897ec5325dd4abe81b5600637a96af0
msgid "English | 简体中文"
msgstr "English | Simplified Chinese"

#: ../../inference/README_INFERENCE.md:4 34649403b6f34d20850313f9fea76af3
msgid "介绍"
msgstr "Introduction"

#: ../../inference/README_INFERENCE.md:6 391bf6c515034d31bfc4ae141acd6eeb
msgid "nndeploy内部推理子模块是一个更加适合模型部署的推理框架。"
msgstr ""
"The nndeploy internal inference submodule is a more model-deployment-"
"friendly inference framework."

#: ../../inference/README_INFERENCE.md:8 35963356fb994f0d891343c0ecdfbf4c
msgid "目前后端算子支持CPU和华为昇腾NPU，未来会逐步扩展支持CUDA、ARM、OpenCL等异构计算平台。"
msgstr ""
"Currently, the backend operators support CPU and Huawei Ascend NPU, with "
"future plans to gradually expand support for CUDA, ARM, OpenCL, and other "
"heterogeneous computing platforms."

#: ../../inference/README_INFERENCE.md:10 412811038d0149c187a76b348ef450b0
msgid ""
"在模型支持方面，现已支持图像分类(如resnet50)、目标检测(如YOLOV11)、图像分割(如RMBG1.4)等主流视觉模型，后续还会扩展支持大语言模型(LLM)、文本图像多模态(如Dit)等热门AI模型。"
msgstr ""
"In terms of model support, it currently includes mainstream vision models "
"such as image classification (e.g., resnet50), object detection (e.g., "
"YOLOV11), image segmentation (e.g., RMBG1.4), and will further expand to "
"support large language models (LLM), text-image multimodal models (e.g., "
"Dit), and other trending AI models."

#: ../../inference/README_INFERENCE.md:12 9390ce284ca3441bbbeb78e3788b8fd3
msgid "架构"
msgstr "Architecture"

#: ../../inference/README_INFERENCE.md ../../inference/README_INFERENCE.md:14
#: 42ac5fecd77c46da9e848ae522d4e8ee b5690c2eee4241e5a921ed4452003f46
msgid "inference_framework_arch"
msgstr "inference_framework_arch"

#: ../../inference/README_INFERENCE.md:17 18dd549904914d61acd94cf53c174917
msgid "为什么要在内部实现一个推理框架"
msgstr "Why implement an inference framework internally"

#: ../../inference/README_INFERENCE.md:19 5e7cd26830ac4fd9ad79f0a92eeedb52
msgid "从模型部署角度出发，推理框架需支持更多功能，满足实际应用需求：我们总结了推理框架需支持的关键功能："
msgstr ""
"From a model deployment perspective, an inference framework needs to support"
" more functionalities to meet practical application requirements: We "
"summarized the key functionalities an inference framework needs to support:"

#: ../../inference/README_INFERENCE.md ../../inference/README_INFERENCE.md:21
#: 2786a563a2a24e95af34901f5bcda5bc 2d7ea57c54b944c1964adc3b4bf1c487
msgid "inference_need_support"
msgstr "inference_need_support"

#: ../../inference/README_INFERENCE.md:23 d55a993dc63c4d3cab927159a6ae0bbc
msgid ""
"nndeploy已具备大量推理相关基础组件：nndeploy在模型部署方面有许多优秀特性。nndeploy背后有大量精心设计和开发工作，为用户提供功能强大、易用、高性能且兼容主流框架的模型推理和部署体验。nndeploy已具备大量推理相关基础组件，因此我们选择先基于华为昇腾生态，开发一个内部推理框架。该框架将从部署角度出发，提供更全面、易用的功能。"
msgstr ""
"nndeploy has already equipped a large number of inference-related basic "
"components: nndeploy excels in model deployment with many outstanding "
"features. Behind nndeploy is a plethora of meticulously designed and "
"developed work, offering users a powerful, easy-to-use, high-performance, "
"and mainstream-framework-compatible model inference and deployment "
"experience. nndeploy has already equipped a large number of inference-"
"related basic components, hence we chose to first develop an internal "
"inference framework based on the Huawei Ascend ecosystem. This framework "
"will start from a deployment perspective, providing more comprehensive and "
"user-friendly functionalities."

#: ../../inference/README_INFERENCE.md:25 3148e7fd0ce844f494f10aea20f371e3
msgid ""
"紧跟大模型时代AI基础设施发展：大模型在各领域展现强大能力，但也对AI基础设施提出更高要求，如更高效内存管理、更强大分布式推理能力、更深度的算子优化等。开发该内部推理框架，将使我们能紧跟大模型时代步伐。"
msgstr ""
"Keeping up with the development of AI infrastructure in the era of large "
"models: Large models demonstrate powerful capabilities across various fields"
" but also impose higher demands on AI infrastructure, such as more efficient"
" memory management, stronger distributed inference capabilities, deeper "
"operator optimization, etc. Developing this internal inference framework "
"will enable us to keep pace with the steps of the large model era."

#: ../../inference/README_INFERENCE.md:27 ee53fd74da234e9e82db27e43c7015ea
msgid "特点"
msgstr "Features"

#: ../../inference/README_INFERENCE.md:29 d9c5ae6da2994b4da86940c187014006
msgid "1. 功能丰富"
msgstr "1. Rich functionalities"

#: ../../inference/README_INFERENCE.md:31 43b25137b9c246a68fd815cf66d2e2b7
msgid "支持手动构图：当图优化手段滞后时，通过手动构图方式，手动把零碎算子融合成一个大的算子，可以提升推理性能。"
msgstr ""
"Support manual graph construction: When graph optimization methods lag, "
"manually combining fragmented operators into a large operator through manual"
" graph construction can improve inference performance."

#: ../../inference/README_INFERENCE.md:32 0de14306b8dd488f87305d1e304b9e4c
msgid ""
"支持加载并图优化量化中间格式QDQ模型：推理框架可以支持加载并图优化QDQ模型，模型量化就可以在训练框架或者ONNX层面完成，推理框架负责加载量化的中间格式QDQ模型，完成后续模型图优化工作。这样就可以解绑量化和推理框架。"
msgstr ""
"Support loading and optimizing quantized intermediate format QDQ models: The"
" inference framework can support loading and optimizing QDQ models, where "
"model quantization can be completed at the training framework or ONNX level,"
" and the inference framework is responsible for loading the quantized "
"intermediate format QDQ models, completing subsequent model graph "
"optimization work. This can decouple quantization from the inference "
"framework."

#: ../../inference/README_INFERENCE.md:33 296820120de64d5ebcd3c445b84cd95d
msgid "支持共享上下文和共享流：打通应用和推理，实现应用和模型推理的输入输出零拷贝以及重叠执行，提升全流程性能。"
msgstr ""
"Support shared context and shared streams: Breaking through the application "
"and inference, achieving zero-copy of inputs and outputs between application"
" and model inference as well as overlapping execution, enhancing the "
"performance of the entire process."

#: ../../inference/README_INFERENCE.md:34 c8a21c185f304bdcb68ad7b415bdb58f
msgid "支持动态形状输入：适配更多的应用场景，支持更多的模型高性能落地，支持动态batch size、动态序列长度、动态图像尺寸"
msgstr ""
"Support dynamic shape input: Adapt to more application scenarios, support "
"more models' high-performance deployment, support dynamic batch size, "
"dynamic sequence length, dynamic image size."

#: ../../inference/README_INFERENCE.md:35 529f84aeb61545ffb55fe3b36ec7653f
msgid "支持多设备算子后端：目前后端算子支持CPU和华为昇腾NPU，未来会逐步扩展支持CUDA、ARM、OpenCL等异构计算平台。"
msgstr ""
"Support multi-device backend operators: Currently, the backend operators "
"support CPU and Huawei Ascend NPU, with future plans to gradually expand "
"support for CUDA, ARM, OpenCL, and other heterogeneous computing platforms."

#: ../../inference/README_INFERENCE.md:37 4fa6305a0aab42b3921eb3c191d6990f
msgid "2. 简单易用"
msgstr "2. Simple and easy to use"

#: ../../inference/README_INFERENCE.md:39 2ac924d895fd4e9995eb067b34ab2b6c
msgid "支持直接读取ONNX模型: 简化流程,帮助工程师快速验证模型的精度和性能。"
msgstr ""
"Support direct reading of ONNX models: Simplify the process, helping "
"engineers quickly verify the model's accuracy and performance."

#: ../../inference/README_INFERENCE.md:40 6c7d041a461b4ffc9476064ff7bbff39
msgid "支持直接读取safetensors格式的模型权重文件: 简化流程,帮助工程师快速验证模型的精度和性能。"
msgstr ""
"Support direct reading of safetensors format model weight files: Simplify "
"the process, helping engineers quickly verify the model's accuracy and "
"performance."

#: ../../inference/README_INFERENCE.md:41 d61ac38687244afbb4a158a70c2c8126
msgid "提供多层次的API：让开发者轻松上手,提供简单易用的高层API，让资深工程师能够进行更精细的性能调优,提供灵活的底层API。"
msgstr ""
"Provide multi-level APIs: Allow developers to get started easily, providing "
"simple and easy-to-use high-level APIs, enabling experienced engineers to "
"perform more detailed performance tuning, offering flexible low-level APIs."

#: ../../inference/README_INFERENCE.md:43 d92aa242366f40e5adff79b520fc90eb
msgid "3. 高性能"
msgstr "3. High performance"

#: ../../inference/README_INFERENCE.md:45 6de6a3fc8c494c70a9f7d041cdcf9d10
msgid "内存复用：基于计算图的高效的内存复用管理机制"
msgstr ""
"Memory reuse: High-efficiency memory reuse management mechanism based on "
"computational graphs."

#: ../../inference/README_INFERENCE.md:46 8bdc1f5c607b4609936f35233250de7d
msgid "多模型推理实例共享内存：通过多模型共享激活值内存技术,多个模型时分复用内存,增加单台机器可以部署的模型数量，实现“多个模型，单份内存”"
msgstr ""
"Multi-model inference instances share memory: Through multi-model shared "
"activation value memory technology, multiple models share memory at "
"different times, increasing the number of models that can be deployed on a "
"single machine, achieving \"multiple models, single memory copy\"."

#: ../../inference/README_INFERENCE.md:47 960ce51f716a462c88db31d74de763be
msgid "图优化：通过对IR/计算图进行一系列优化，如算子融合、常量折叠、公共子表达式消除等，提高模型推理性能。"
msgstr ""
"Graph optimization: Through a series of optimizations on IR/computational "
"graphs, such as operator fusion, constant folding, elimination of common "
"subexpressions, etc., to improve model inference performance."

#: ../../inference/README_INFERENCE.md:48 957a6430852046e2ab8f33f1f380e6c1
msgid "算子优化：针对不同硬件平台，如CPU、华为昇腾NPU等，对算子进行深度优化，充分利用硬件特性，实现高效计算。"
msgstr ""
"Operator optimization: For different hardware platforms, such as CPU, Huawei"
" Ascend NPU, etc., deeply optimize operators, fully utilizing hardware "
"characteristics to achieve high-efficiency computation."

#: ../../inference/README_INFERENCE.md:50 8a5a59a63c7845608c7cb6f1c37bde75
msgid "关键子模块"
msgstr "Key submodules"

#: ../../inference/README_INFERENCE.md:52 8e3420d7b07d4a22832b6926c5c31feb
msgid ""
"模型中间表示(IR)：一组精心设计的数据结构，用于描述模型的结构和权重信息。它是连接模型解释、图优化、计算图构建等推理框架各个关键模块的核心数据结构。IR作为推理框架内部的统一模型表示，在简化框架设计的同时，也方便了各模块之间的交互。"
msgstr ""
"Model Intermediate Representation (IR): A set of carefully designed data "
"structures used to describe the structure and weight information of a model."
" It is the core data structure connecting model interpretation, graph "
"optimization, computational graph construction, and other key submodules of "
"the inference framework. As the unified model representation inside the "
"inference framework, IR simplifies the framework design while facilitating "
"interaction between modules."

#: ../../inference/README_INFERENCE.md:54 435301307c3848d7b34b18af07178c2f
msgid ""
"模型解释(Model "
"Interpreter)：是将训练框架模型文件转换为推理框架自定义模型文件的模块。目前支持将onnx模型文件转换为自定义模型文件（模型结构文件JSON,模型权重文件safetensors），也支持自定义模型文件与自定义IR之间的相互转换。通过模型解释模块，可以实现不同格式的模型在推理框架内部的统一表示，简化了推理框架的设计。"
msgstr ""
"Model Interpreter: A module that converts training framework model files "
"into custom model files for the inference framework. Currently, it supports "
"converting onnx model files into custom model files (model structure file "
"JSON, model weight file safetensors), and also supports mutual conversion "
"between custom model files and custom IR. Through the model interpreter "
"module, different format models can achieve unified representation inside "
"the inference framework, simplifying the design of the inference framework."

#: ../../inference/README_INFERENCE.md:56 d13970d3575d4164aaf230f0585bce1d
msgid "计算图(DAG)：由算子(Operator)和张量(Tensor)构成的带执行属性的有向无环图，通过IR和配置参数构建而成。"
msgstr ""
"Computational Graph (DAG): A directed acyclic graph with execution "
"attributes composed of operators and tensors, constructed through IR and "
"configuration parameters."

#: ../../inference/README_INFERENCE.md:58 468d3dd5957248cc8159619f760f3aa0
msgid ""
"运行时（Runtime）：负责执行计算图，支持模型推理和单算子执行两种模式。它基于带执行属性的有向无环图，为计算图分配所需资源，写入输入，执行计算图，最终得到推理结果。"
msgstr ""
"Runtime: Responsible for executing the computational graph, supporting two "
"modes: model inference and single operator execution. Based on the directed "
"acyclic graph with execution attributes, it allocates required resources for"
" the computational graph, writes inputs, executes the computational graph, "
"and finally obtains the inference results."

#: ../../inference/README_INFERENCE.md:60 53fe70758ead491184f6ea5f4d03e7ed
msgid ""
"Ascend算子库（Ascend Operator "
"Library）：是基于华为昇腾平台的丰富算子库，包括NN、BLAS、AIPP、DVPP等多种类型的高性能算子。算子与计算图紧密关联，是推理框架的核心组成部分。"
msgstr ""
"Ascend Operator Library: A rich operator library based on the Huawei Ascend "
"platform, including various types of high-performance operators such as NN, "
"BLAS, AIPP, DVPP. Operators are closely related to the computational graph, "
"forming the core component of the inference framework."

#: ../../inference/README_INFERENCE.md:62 2be4771cd34e49c19d17d3a0de51b9c6
msgid ""
"图优化（Graph "
"Optimization）：在模型推理的各个阶段对IR或计算图进行优化，以提升模型推理性能。它包括算子融合、常量折叠、公共子表达式消除、Transpose消除、恒等算子消除等多种优化手段，并结合硬件特性和具体模型特点选择合适的优化策略。"
msgstr ""
"Graph Optimization: Optimizes IR or computational graphs at various stages "
"of model inference to improve model inference performance. It includes "
"multiple optimization techniques such as operator fusion, constant folding, "
"elimination of common subexpressions, transpose elimination, identity "
"elimination, and combines hardware characteristics and specific model "
"features to select appropriate optimization strategies."

#: ../../inference/README_INFERENCE.md:64 cdaf292b9af7477fb7c59c92a266291a
msgid ""
"内存优化（Memory Optimization）：通过分析计算图中Tensor的生命周期，让生命周期不重叠的Tensor进行内存复用，以减少内存占用。"
msgstr ""
"Memory Optimization: By analyzing the lifecycle of Tensors in the "
"computation graph, Tensors with non-overlapping lifecycles are reused in "
"memory to reduce memory usage."

#: ../../inference/README_INFERENCE.md:66 631fa4c67a6348719d517fad485d4958
msgid ""
"并行优化（Parallel Optimization）：通过合理利用硬件资源的并行能力来提升模型推理性能，包括数据并行、流水线并行、张量并行等并行模式。"
msgstr ""
"Parallel Optimization: Enhances model inference performance by leveraging "
"hardware resources' parallel capabilities, including data parallelism, "
"pipeline parallelism, tensor parallelism, and other parallel models."

#: ../../inference/README_INFERENCE.md:68 89e2d1e7689e4c2db3d5b904bcbf3fbc
msgid ""
"Ascend C算子开发（Ascend C Operator "
"Development）：是基于华为昇腾平台的自定义算子开发语言，支持用户根据实际需求自定义和优化算子实现。"
msgstr ""
"Ascend C Operator Development: A custom operator development language based "
"on Huawei's Ascend platform, allowing users to customize and optimize "
"operator implementations according to actual needs."
