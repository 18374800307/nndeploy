# 常见问题

## 问题一 来自 nndeploy交流群
### Q: 我想了解一下：onnx runtime也可以选择不同的execution provider: https://onnxruntime.ai/docs/execution-providers/ nndeploy与其思路的差别主要在哪里呢？ 
### A: ONNXRuntime与nndeploy最大的区别是：ONNXRuntime首先是一个的推理框架，然后可以接入其他推理框架，nndeploy明确定位就是一个多端部署框架。这个会带来的差异主要是如下两点
- ONNXRuntime为了保证其他推理框架兼容其自身，在功能上会对其他推理框架有一定程度的阉割（比如TNN、mnn操作推理框架内部分配的输入输出、OpenVINO CPU-GPU异构模式、模型量化等等），导致模型通过EP只是能跑起来，性能应该不好。nndeploy希望类似：你搞了很久的trt，然后你就trt搭建了一个自己的小型框架，可以完全操纵trt;
- ONNXRuntime强绑定ONNX以及protobuf，会导致包体积变大，尤其是对于移动端不友好，对于不能直接读取onnx模型文件的框架应该也不太友好，还有就是现在很多框架都是pt->自定义模型文件（例如ncnnn\mnn），以达到极致的性能

总结而言，都是调用第三方推理框架，使用nndeploy功能更全面、性能更好、体验更丝滑。

## 问题二 来自 nndeploy交流群
### Q: nndeploy相对于openmmlab的mmdeploy以及paddle的fastdeploy会有什么优势？
### A: 非常开心可以和openmmlab的mmdeploy以及paddle的fastdeploy放在一起比较，毫无疑问，openmmlab的mmdeploy以及paddle的fastdeploy都是行业巨擘，有非常大影响力，据我所知都有非常多公司在使用这个两个库。因为我们是nndeploy的开发者，所以我们这里主要谈谈我们的优势。
- 更加开放。目前fd家重点是paddle的模型仓库以及主推自家推理框架，mmdp家主要是openmmlab的模型仓库以及主推自家推理框架，而nndeploy积极集成各家的推理框架，不设限的部署热门的开源模型。nndeploy想跳出这个"特定优先支持"，做一个更加通用普适的部署框架
- 更加易用，在多端推理的基础功能的基础上nndeploy还做了设备管理，让你可以有统一的方式操作内存以及执行流等；还通过有向无环图来管理模型部署的前处理、推理、后处理
- 更加适合部署多模型，nndeploy以有向无环图的形式来实现多个模型的部署，通过这种方式来部署多模型算法，可以少写大量的业务代码，模块性以及鲁棒性都会更好。
- 推理框架的高性能抽象：每个推理框架也都有其各自的特性，需要足够尊重以及理解这些推理框架，才能在抽象中不丢失推理框架的特性，并做到统一的使用的体验。nndeploy可配置第三方推理框架绝大部分参数，保证了推理性能。可直接操作理框架内部分配的输入输出，实现前后处理的零拷贝，提升模型部署端到端的性能。我们正在实现更多性能方面的优化，比如在图的基础上加上线程池、内存池、高性能算子库等，未来性能方面的功能会逐渐更加完善
- nndeploy更加专注模型c++部署，更轻量化一些

## 问题三 来自 nndeploy交流群
### Q: nndeploy与triton的区别 
### A: 主要有以下四点
- triton侧重于对模型推理阶段的统一管理；nndeploy关注模型的端到端（包括前后处理）部署的统一管理；
- triton屏蔽了内部推理框架接口，注重于对用户的推理框架黑盒，假定受众为不了解推理框架使用的普通用户；nndeploy注重于屏蔽不同推理框架的使用差异，但为开发人员保留对推理框架的控制能力，可以服务于高级用户，只是侧重于减少代码开发工作量，关注代码在不同推理框架的兼容性；
- triton的优化主要是推理阶段的资源/任务调度，nndeploy的优化来自于全流程，包括资源/任务调度、前后处理与模型推理中的全流程内存优化、模型优化等；
- 目前triton将所关注的功能封装为server，nndeploy当前版本暂未实现（未来将会有）

## 问题四 来自内部讨论
### Q: 当前模型部署有哪些痛点
### A: 主要有以下四点
- 现在业界尚不存在各方面都远超其同类产品的推理框架，不同推理框架在不同平台、硬件下分别具有各自的优势。例如，在 `Linux` + `NVidia` 显卡机器推理，`TensorRT` 是性能最好的推理框架；在 `Windows` + `x86 CPU` 机器推理，`OpenVINO` 是性能最好的推理框架；在 `ARM Android` 下，有 `ncnn`、`MNN`、`TFLite`、TNN等一系列选择。
- 不同的推理框架有不一样的推理接口、推理配置等 API，针对不同推理框架都需要写一套代码，这对模型部署工程师而言，将带来较大学习成本、开发成本、维护成本
- 模型部署不仅仅只有模型推理，还有前处理、后处理，推理框架往往只提供模型推理的功能
- 目前很多场景是需要由多个模型组合解决该业务问题（例如stable diffusion、老照片修复、人脸识别等等），直接采用推理框架的原生接口，会有大量且低效的业务代码编写

## 与HelloGithub蛋佬讨论交流

### 1 框架简介

如下是三个开源框架在GitHub主页上的介绍

**1.1 nndeploy**：nndeploy是一款模型端到端部署框架。以多端推理以及基于有向无环图模型部署为基础，致力为用户提供跨平台、简单易用、高性能的模型部署体验。

**1.2 fastdeploy**：FastDeploy是一款全场景、易用灵活、极致高效的AI推理部署工具， 支持云边端部署。提供超过 160+ Text，Vision， Speech和跨模态模型开箱即用的部署体验，并实现端到端的推理性能优化。包括 物体检测、字符识别（OCR）、人脸、人像扣图、多目标跟踪系统、NLP、Stable Diffusion文图生成、TTS 等几十种任务场景，满足开发者多场景、多硬件、多平台的产业部署需求。

**1.2 mmdeploy**：MMDeploy 是 OpenMMLab 模型部署工具箱，为各算法库提供统一的部署体验。基于 MMDeploy，开发者可以轻松从训练 repo 生成指定硬件所需 SDK，省去大量适配时间。

### 2 框架对比

| 名称       | 易用性 | 性能 | 并行   | 开放性 | 模型支持 | 扩展性 |
| ---------- | ------ | ---- | ------ | ------ | -------- | ------ |
| nndeploy   | 高     | 高   | 支持   | 高     | 低       | 低     |
| fastdeploy | 中     | 中   | 不支持 | 中     | 高       | 高     |
| mmdeploy   | 中     | 中   | 不支持 | 中     | 高       | 高     |

**2.1 易用性**：基于有向无环图部署模型，模型部署的全流程（前处理->推理->后处理）简单明了；构建推理模板Infer，可屏蔽模型的复杂性；高效解决多模型的复杂场景，通过支持图中嵌入图灵活且强大的功能，将大问题拆分为小问题，通过组合的方式快速解决多模型的复杂场景问题；可快速构建demo

**2.2 性能**：对推理框架的高性能抽象；高效的线程池

**2.3 并行**：基于有向无环图部署模型，支持多种并行模式，进一步提升模型部署的性能

**2.4 开放性**：目前fastdeploy重点是paddle的模型仓库以及主推自家推理框架，mmdeploy主要是openmmlab的模型仓库以及主推自家推理框架，而nndeploy积极集成各家的推理框架，不设限的部署热门的开源模型。nndeploy想跳出这个"特定优先支持"，做一个更加通用普适的部署框架

**2.5 模型支持**：目前nndeploy支持模型较少；fastdeploy支持paddle的模型仓库中绝大部分模型；mmdeploy支持openmmlab的模型仓库中绝大部分模型

**2.6 扩展性**：nndeploy有更加良好的架构，可扩展为推理框架，也可扩展支持分布式的推理部署


## 关于我

**推理框架**：在Android Tee(可信任环境)下，写过一款纯C的推理框架

**部署框架**：从0到1写过一款部署框架，目前已服务数百个模型上线

**模型部署**：完成过数十个CV类模型的在多端的部署

**检测追踪**：对传统检测追踪算法有一定的实战经验

**开源框架**：
- 详细阅读过tnn、mnn、tengine、nnlib四个知名的的推理框架
- 用过openvin、tensorrt、onnxruntime等推理框架
- onnx模型优化器: onnxoptimizer、onnxsim

## 开源的目的

**尝试解决部署的痛点问题**：一直在从事模型部署、推理、高性能计算相关领域，发现了模型部署一些痛点问题，想尝试去解决这些问题

**技术上的追求**：基于nndeploy，可以实现在AI Infra上的一些想法，能跟上AI发展的节奏

**拓宽知识边界**：认识一些朋友，通过交流拓宽知识边界

**影响力**：或许能对业界产生一些帮助，自己也可以收获一些影响力


