
# nndeploy技术分享

## 多端部署实际案例 - 智能抠图

## 业界的痛点问题

- **推理框架的碎片化**：现在业界尚不存在各方面都远超其同类产品的推理框架，不同推理框架在不同平台、硬件下分别具有各自的优势。例如，在`NVidia` 显卡机器推理，`TensorRT` 是性能最好的推理框架；在`x86 CPU` 机器推理，`OpenVINO` 是性能最好的推理框架；在苹果生态下，`coreml`是性能最好的推理框架；在`ARM Android` 下，有 `ncnn`、`MNN`、`TFLite`、`TNN`等一系列选择；在`瑞芯微`下，`RKNN`是性能最好的推理框架；。（总结而言：在具体硬件下，就采用硬件公司推出的推理框架）
  
- **多个推理框架 的 学习成本、开发成本、维护成本**：不同的推理框架有不一样的推理接口、推理配置、Tensor等等，针对不同推理框架都需要写一套代码，这对模型部署工程师而言，将带来较大学习成本、开发成本、维护成本
  
- **模型本身的多样性**：针对不同的模型，又有很多差异性，例如**单输入、多输入、单输出、多输出、静态形状输入、动态形状输入、静态形状输出、动态形状输出、**一系列不同，当与内存零拷贝优化结合的时候（直接操作推理框架内部分配输入输出），通常只有具备丰富模型部署经验的工程师才能快速找到最优解
  
以下是结合了模型特性、描述、TensorRT手动构图以及实际算法例子的表格：

| 模型特性     | 描述                               | TensorRT手动构图                                                                             | 实际算法例子                                                                                  |
| ------------ | ---------------------------------- | -------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------- |
| 单输入       | 模型只有一个输入张量。             | 确保`NetworkDefinition`中只有一个输入节点。                                                  | 图像分类模型ResNet，它接收单张图像作为输入，并输出图像的分类结果。                            |
| 多输入       | 模型有多个输入张量。               | 在`NetworkDefinition`中定义多个输入节点，并在推理后处理时获取所有输入。                      | 划痕修复模型，它接收原始图像以及划痕检测mask作为输入                                          |
| 单输出       | 模型只有一个输出张量。             | 确保`NetworkDefinition`中只有一个输出节点。                                                  | 图像检测模型YOLOv5，将后处理融合到模型内部                                                    |
| 多输出       | 模型有多个输出张量。               | 在`NetworkDefinition`中定义多个输出节点，并在推理后处理时获取所有输出。                      | 图像检测模型YOLOv5，将后处理不融合到模型内部                                                  |
| 静态形状输入 | 输入张量的形状在推理前已知且不变。 | 在`BuilderConfig`中设置固定的输入形状。                                                      | 上述模型基本都为静态输入模型                                                                  |
| 动态形状输入 | 输入张量的形状在推理时可能变化。   | 使用`IOptimizationProfile`定义输入张量的动态形状，并在`ExecutionContext`中动态设置输入形状。 | 自适应的图像超分辨率模型，它能够接收不同尺寸的低分辨率图像作为输入，并输出高分辨率的图像。    |
| 静态形状输出 | 输出张量的形状在推理前已知且不变。 | 不需要在推理时动态调整。                                                                     | 除动态形状输入模型外，上述模型基本都为静态输出模型                                            |
| 动态形状输出 | 输出张量的形状在推理时可能变化。   | 需要在推理后处理时动态获取输出形状，并据此处理输出数据。                                     | 机器翻译模型，如Transformer，它接收任意长度的文本作为输入，并输出相应长度的目标语言翻译文本。 |


- **模型高性能的前后处理**：模型部署不仅仅只有模型推理，还有前处理、后处理，推理框架往往只提供模型推理的功能
  
- **多模型的复杂场景**：目前很多场景是需要由多个模型组合解决该业务问题（例如老照片修复、stable diffusion等等），没有部署框架的支持，会有大量业务代码、模型耦合度高、灵活性差、代码不适合并行等等问题（出bug、可维护性）

## 介绍

`nndeploy`是一款模型端到端部署框架。以`多端推理`以及`基于有向无环图模型部署`为内核，致力为用户提供跨平台、简单易用、高性能的模型部署体验。

## 架构

![Architecture](../../image/architecture.jpg)

## 特点

### 1. 开箱即用的算法

目前已完成 [YOLOV5](https://github.com/ultralytics/yolov5)、[YOLOV6](https://github.com/meituan/YOLOv6)、[YOLOV8](https://github.com/ultralytics) 等模型的部署，可供您直接使用，后续我们持续不断去部署其它开源模型，让您开箱即用

| model                                                       | Inference                         | developer                                                                                            | remarks |
| :---------------------------------------------------------- | :-------------------------------- | :--------------------------------------------------------------------------------------------------- | :-----: |
| [YOLOV5](https://github.com/ultralytics/yolov5)             | TensorRt/OpenVINO/ONNXRuntime/MNN | [02200059Z](https://github.com/02200059Z)、[Always](https://github.com/Alwaysssssss)                 |         |
| [YOLOV6](https://github.com/meituan/YOLOv6)                 | TensorRt/OpenVINO/ONNXRuntime     | [02200059Z](https://github.com/02200059Z)、[Always](https://github.com/Alwaysssssss)                 |         |
| [YOLOV8](https://github.com/ultralytics)                    | TensorRt/OpenVINO/ONNXRuntime/MNN | [02200059Z](https://github.com/02200059Z)、[Always](https://github.com/Alwaysssssss)                 |         |
| [SAM](https://github.com/facebookresearch/segment-anything) | ONNXRuntime                       | [youxiudeshouyeren](https://github.com/youxiudeshouyeren)、[Always](https://github.com/Alwaysssssss) |         |

### 2. 支持跨平台和多推理框架

**一套代码多端部署**：通过切换推理配置，一套代码即可完成模型`跨多个平台以及多个推理框架`部署

当前支持的推理框架如下：

| Inference/OS                                                                     | Linux | Windows | Android | MacOS |  IOS  | developer                                                                          | remarks |
| :------------------------------------------------------------------------------- | :---: | :-----: | :-----: | :---: | :---: | :--------------------------------------------------------------------------------- | :-----: |
| [TensorRT](https://github.com/NVIDIA/TensorRT)                                   |   √   |    -    |    -    |   -   |   -   | [Always](https://github.com/Alwaysssssss)                                          |         |
| [OpenVINO](https://github.com/openvinotoolkit/openvino)                          |   √   |    √    |    -    |   -   |   -   | [Always](https://github.com/Alwaysssssss)                                          |         |
| [ONNXRuntime](https://github.com/microsoft/onnxruntime)                          |   √   |    √    |    -    |   -   |   -   | [Always](https://github.com/Alwaysssssss)                                          |         |
| [MNN](https://github.com/alibaba/MNN)                                            |   √   |    √    |    √    |   -   |   -   | [Always](https://github.com/Alwaysssssss)                                          |         |
| [TNN](https://github.com/Tencent/TNN)                                            |   √   |    √    |    √    |   -   |   -   | [02200059Z](https://github.com/02200059Z)                                          |         |
| [ncnn](https://github.com/Tencent/ncnn)                                          |   -   |    -    |    √    |   -   |   -   | [Always](https://github.com/Alwaysssssss)                                          |         |
| [coreML](https://github.com/apple/coremltools)                                   |   -   |    -    |    -    |   √   |   -   | [JoDio-zd](https://github.com/JoDio-zd)、[jaywlinux](https://github.com/jaywlinux) |         |
| [paddle-lite](https://github.com/PaddlePaddle/Paddle-Lite)                       |   -   |    -    |    -    |   -   |   -   | [qixuxiang](https://github.com/qixuxiang)                                          |         |
| [AscendCL](https://www.hiascend.com/zh/)                                         |   √   |    -    |    -    |   -   |   -   | [CYYAI](https://github.com/CYYAI)                                                  |         |
| [RKNN](https://www.rock-chips.com/a/cn/downloadcenter/BriefDatasheet/index.html) |   √   |    -    |    -    |   -   |   -   | [100312dog](https://github.com/100312dog)                                          |         |


**Notice:** TFLite, TVM, OpenPPL, sophgo, Horizon正在开发中，我们正在努力覆盖绝大部分的主流推理框架

### 3. 简单易用

- **基于有向无环图部署模型**： 将 AI 算法端到端（前处理->推理->后处理）的部署抽象为有向无环图 `Graph`，前处理为一个 `Node`，推理也为一个 `Node`，后处理也为一个 `Node`
 
- **推理模板Infer**： 基于`多端推理模块Inference` + `有向无环图节点Node`再设计功能强大的`推理模板Infer`，Infer推理模板可以帮您在内部处理不同的模型带来差异，例如**单输入、多输入、单输出、多输出、静态形状输入、动态形状输入、静态形状输出、动态形状输出**一系列不同
 
- **高效解决多模型的复杂场景**：在多模型组合共同完成一个任务的复杂场景下（例如老照片修复），每个模型都可以是独立的Graph，nndeploy的有向无环图支持`图中嵌入图`灵活且强大的功能，将大问题拆分为小问题，通过组合的方式快速解决多模型的复杂场景问题

- **快速构建demo**：对于已部署好的模型，需要编写demo展示效果，而demo需要处理多种格式的输入，例如图片输入输出、文件夹中多张图片的输入输出、视频的输入输出等，通过将上述编解码节点化，可以更通用以及更高效的完成demo的编写，达到快速展示效果的目的（目前主要实现了基于OpneCV的编解码节点化）

### 4. 高性能

- **推理框架的高性能抽象**：每个推理框架也都有其各自的特性，需要足够尊重以及理解这些推理框架，才能在抽象中不丢失推理框架的特性，并做到统一的使用的体验。`nndeploy` 可配置第三方推理框架绝大部分参数，保证了推理性能。可直接操作推理框架内部分配的输入输出，实现前后处理的零拷贝，提升模型部署端到端的性能。

- **线程池**：提高模型部署的并发性能和资源利用率（thread pool）。此外，还支持CPU端算子自动并行，可提升CPU算子执行性能（parallel_for）。
  
- **内存池**：完成后可实现高效的内存分配与释放(TODO)
  
- **一组高性能的算子**：完成后将加速您模型前后处理速度(TODO)

### 5. 并行

- **串行**：按照模型部署的有向无环图的拓扑排序，依次执行每个节点。

- **流水线并行**：在处理多帧的场景下，基于有向无环图的模型部署方式，可将前处理 `Node`、推理 `Node`、后处理 `Node`绑定三个不同的线程，每个线程又可绑定不同的硬件设备下，从而三个`Node`可流水线并行处理。在多模型以及多硬件设备的的复杂场景下，更加可以发挥流水线并行的优势，从而可显著提高整体吞吐量。

- **任务并行**：在多模型以及多硬件设备的的复杂场景下，基于有向无环图的模型部署方式，可充分挖掘模型部署中的并行性，缩短单次算法全流程运行耗时

- **上述模式的组合并行**：在多模型、多硬件设备以及处理多帧的复杂场景下，nndeploy的有向无环图支持图中嵌入图的功能，每个图都可以有独立的并行模式，故用户可以任意组合模型部署任务的并行模式，可充分发挥硬件性能。

## 源码浅分析

### 多端推理框架

### 基于有向无环图的模型部署

+ **Parallel**: 并行模块。基于有向无环图的并行模块，任务并行可缩短单次算法全流程运行耗时，流水线并行可提高算法全流程运吞吐量。

+ **Directed Acyclic Graph**：有向无环图子模块。模型端到端的部署流程可抽象成 `3` 个子块：**模型前处理+>模型推理+>模型推理**，这是一个非常典型的有向无环图，对于多模型组合的算法而言，是更加复杂的的有向无环图，直接写业务代码去串联整个过程不仅容易出错，而且还效率低下，采用有向无环图的方式可以极大的缩减业务代码的编写。

+ **Process Template**：前后处理模板以及推理子模板。我们希望还再可以简化您的部署流程，因此在模型端到端的部署的**模型前处理+>模型推理+>模型推理**的三个过程中，我们进一步设计模板。尤其是在推理模板上面花了足够多的心思，针对不同的模型，又有很多差异性，例如**单输入、多输出、静态形状输入、动态形状输入、静态形状输出、动态形状输出、是否可操作推理框架内部分配输入输出**等等一系列不同，只有具备丰富模型部署经验的工程师才能快速解决上述问题，故我们基于多端推理模块 `Inference` + 有向无环图节点 `Node` 再设计功能强大的**推理模板Infer**，这个推理模板可以帮您在内部处理上述针对模型的不同带来的差异。
  
+ **Resouce Pool**：资源管理子模块。正在开发线程池以及内存池（这块是 `nndeploy` 正在火热开发的模块，期待大佬一起来搞事情）。线程池可实现有向无环图的流水线并行，内存池可实现高效的内存分配与释放。

+ **Inference**：多端推理子模块（ `nndeploy` 还需要集成更多的推理框架，期待大佬一起来搞事情）。提供统一的推理接口去操作不同的推理后端，在封装每个推理框架时，我们都花了大量时间去理解并研究各个推理框架的特性，例如 `TensorRT` 可以使用外存推理，`OpenVINO` 有高吞吐率模式、`TNN` 可以操作内部分配输入输出等等。我们在抽象的过程中不会丢失推理框架的特性，并做到统一的使用的体验，还保证了性能。

+ **OP**：高性能算子模块。我们打算去开发一套高性能的前后处理算子（期待有大佬一起来搞事情），提升模型端到端的性能，也打算开发一套 `nn` 算子库或者去封装 `oneDNN`、`QNN` 等算子库（说不定在 `nndeploy` 里面还会做一个推理框架呀）

+ **Data Container**：数据容器子模块。推理框架的封装不仅推理接口的 API 的封装，还需要设计一个 Tensor，用于去与第三方推理框架的 Tensor 进行数据交互。 `nndeploy` 还设计图像处理的数据容器 Mat，并设计多设备的统一内存 Buffer。

+ **Device**：设备管理子模块。为不同的设备提供统一的内存分配、内存拷贝、执行流管理等操作。