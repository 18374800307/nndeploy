
我们现在切到 IDE 来讲解IR 部分的代码具体的实现。IR 代码的位置在根目录下的 framework 目录。我们先点开 include 目录，看一下头文件，找到 ir 目录下的 ir.h，这是我们要讲的一个文件，还有 op_param.h 是另外一个文件，对应的源文件在 source 目录。

首先看一下 ir.h，针对IR这个模块，先讲成员变量，然后再讲成员函数，因为成员函数是辅助成员变量做事情的，把成员变量讲清楚了，那对应的函数其实你也大概能知道它到底怎么回事。

我们整个 NNDeploy 的 IR 是很大程度借鉴了 ONNX。

我们首先看最顶层的数据结构ModelDesc，我们先看一下 ModelDesc 的成员变量，


首先这个模型需要名称，所以我们有一个 string name。
模型会有很多元数据，比如说它的生产者是谁？它在什么训练框架训出来的，所以我们有一个 unordered_map 来存放一些 metadata。

整个模型会有输入，他可能是多个输入，所以这里是vector的形式，这个输入是一种特殊的结构，我们把他设计为了ValueDesc，等下我们会跳到 ValueDesc 去看一下它具体会有哪些信息。然后模型会有输出，它跟输入是同样的数据结构。

模型是由一系列算子构成的，这一列算子由 vector 来存储。描述算子的具体信息由 OpDesc 决定。

然后是描述模型的权重，模型的单个权重用 tensor 表示，权重需要与算子关联，我们把权重设为一个 map 的形式，string 和 tensor 的 map 形式。这样 tensor 与算子关联就会更加直接，因为 OpDesc 只要存放这个权重的名字即可，算子和权重就关联起来了。

好了，我们对最顶层的数据结构有了基本的认识，现在来看一下 ValueDesc。主要是服务模型的输入输出，

以模型的输入为例，它必须有一个名称，就像我们使用 netron 打开模型时看到的那样，输入都有名称。模型的输入还会有那些信息呢，例如输入的数据类型（fp32呢，还是fp16呢）、输入的形状多大呢，以 YOLOv11s 为例，其默认输入类型是fp32，输入形状是 1 × 3 × 600 × 600。输出跟输入的要求一致。

现在又很多动态shape的模型，你可以在后续在线推理时确定其形状，因此 shape 是可选的。可以做形状推理，也可以做类型推理，所以其类型也是可选的。

讲完了 ValueDesc，我们再跳回 ModelDesc 这个最顶层数据结构。我们再开始讲 OpDesc。我们同样跳进去看一下。首先还是先讲成员变量。算子会有名称，算子会有类型，是卷积类型还是 ReLU 类型？

有了名字和 op_type，算子会有输入和输出。对于某些算子而言，它可以有多个输入，对于某些算子而言，它可以有多个输出，所以输入输出是 vector 的 string 形式。

这里会有一个问题，为什么算子的输入是用 string 来表示？有了名字和 op_type，那算子会有输入算子会有输出。对于某些算子而言，它可以有多个输入，对于某些算子而言，它可以有多个输出，所以输入输出是 vector 的 string 形式。那这里会有一个问题，为什么算子的输入是用 string 来表示？这里我们详细解释一下具体原因。在 IR 阶段，它并不涉及执行，我们并不需要为输入输出分配实际的数据。我们只需要全局有一个唯一的标识符即可。那全局的唯一标识符什么是一种最划算的选择呢？那 string 就是一个最划算的选择，最划算、最易读的选择。所以在 IR 阶段，我们算子的输入输出通过 string 来描述。

那这个算子如何关联我们前面提到的那个权重呢？我们以卷积为例，卷积它有 weight，我们是把卷积的权重当成这个算子的输入来看待。那它的某个 input 就是那个权重的名字。这个权重的名字又关联到了 ModelDesc的成员变量weights中，然后这样就绑定了算子和权重。

通过这个 input output 如何表达计算图呢？这里举个例子，我们可以看到对于卷积而言，它的输入是 input 和 weight，它的输出是 output，那对于下一个算子 ReLU 而言，它的输入 inputs 是这个卷积的 output，那这样子卷积和 ReLU 就关联起来了。

有时候我们需要在ir阶段调试算子的输入输出的类型以及形状，在 IR 阶段就可以有 data type 的信息，就数据类型的信息，还可以有形状的信息，我们可以让这个 输入输出 和 ModelDesc的成员变量value关联祁连。这样算子会具有更丰富的信息，更有利于去调试你的 IR。

算子的描述还需要描述参数，这里用op_param的智能来表示。OpDesc一旦构建起来，就会根据op_type来创建一个特定的算子参数。有些没有参数的算子，例如卷积，这个指针就为空。具体算子参数的实现在op_param中，我们快速过一篇卷积的参数，例如这里的滑窗窗口的大小，padding的大小，这里有一个值得注意的点是，卷积其实可以很多激活函数融合，nndeploy再ir层面提供了融合的可能，我们这里又融合进来的算子类型，融合的算子的参数。

我们讲完了整个 ModelDesc相关的的成员变量。我们再来看一些成员函数。对于 IR 而言，它需要序列化为权重文件，权重文件需要被反序列化为 IR，

所以 IR 需要支持的函数是序列化和反序列化。ModelDesc提供嘞序列化以及反序列化成员函数，其中模型结构序列化为json文件，模型权重序列化为safetensors文件，后续我们在模型解释章节会详细讲我们为什么选择 JSON 和 safetensors 的格式。

在跳到ValueDesc去看下他相关的接口，同样也是序列化和反序列的成员函数

再跳到OpDesc看下相关接口，还是序列化和反序列的成员函数

整个的 ir 已经能够表达模型结构和模型权重。ModelDesc的核心数据以标准库为主，尽可能少的三方依赖数据，然后是尽可能地紧凑，比如说我们算子的输入输出是 string，通过分层的结构设计，序列化与反序列也较为便捷。

好了，我们对 IR 代码实现的部分的介绍，到这里就结束了。